
# LW3

## Задание
1. Реализовать систему классификации согласно описанию, используя не 
менее трех различных архитектур нейронной сети.
2. Сравнить качество работы, скорость и количество потребляемой памяти 
для каждой архитектуры.
3. Сделать отчёт в виде readme на GitHub, там же должен быть выложен 
исходный код.

## Модели
### AlexNet (2012)
AlexNet — это название архитектуры сверточной нейронной сети (CNN), разработанной Алексом Крижевским в сотрудничестве с Ильей Суцкевером и Джеффри Хинтоном.

AlexNet участвовала в крупномасштабном конкурсе визуального распознавания ImageNet 30 сентября 2012 г.
Сеть достигла ошибки топ-5 в 15,3%, что более чем на 10,8 процентных пункта ниже, чем у занявшего второе место. 
Основной результат исходной статьи заключался в том, что глубина модели была важна для ее высокой производительности, 
что требовало больших вычислительных ресурсов, но стало возможным благодаря использованию графических процессоров (GPU) во время обучения.
### ResNet (2015)
Остаточная нейронная сеть (ResNet) представляет собой искусственную нейронную сеть (ИНС). 
Это вариант HighwayNet без шлюза или с открытым шлюзом, первая работающая очень глубокая нейронная сеть с прямой связью с сотнями слоев, 
намного глубже, чем предыдущие нейронные сети. Пропускные соединения или ярлыки используются для перехода через некоторые слои (HighwayNets также могут сами узнавать веса пропуска через дополнительную матрицу весов для своих ворот). 
Типичные модели ResNet реализованы с двух- или трехуровневыми пропусками, которые содержат нелинейности (ReLU) и нормализацию пакетов между ними. 
Модели с несколькими параллельными пропусками называются DenseNets. В контексте остаточных нейронных сетей неостаточную сеть можно описать как простую сеть.
### DenseNet (2017) 
DenseNet (Densely Connected Convolutional Network) была предложена в 2017 году. 
Успех ResNet (Deep Residual Network) позволил предположить, что укороченное соединение в CNN позволяет обучать более глубокие и точные модели. 
Авторы проанализировали это наблюдение и представили компактно соединенный (dense) блок, который соединяет каждый слой с каждым другим слоем. 
Важно отметить, что, в отличие от ResNet, признаки («фичи») прежде чем они будут переданы в следующий слой не суммируются,
а конкатенируются (объединяются, channel-wise concatenation) в единый тензор. 
При этом количество параметров сети DenseNet намного меньше, чем у сетей с такой же точностью работы. 
Авторы утверждают, что DenseNet работает особенно хорошо на малых наборах данных.

## Результат
На 50 огурцах нейронные сети показали следующие результаты:


| Сеть  | AlexNet | ResNet101 | DenseNet121 |
|-------|---------|-----------|-------------|
| top1  | 23      | 30        | 29          |
| top5  | 11      | 15        | 10          | 
| Total | 34      | 45        | 39          |


По времени наблюдались следующие показатели

| Сеть          | AlexNet | ResNet101 | DenseNet121 |
|---------------|---------|-----------|-------------|
| Время на кадр | 0.073   | 0.340     | 0.169       | 

AlexNet как была, так и остаеться самой быстрой при достаточной точности.
Лучше всего себя показала ResNet101, которая смогла определить 45 огурцов из 50

## Источники
- https://habr.com/ru/post/498168/
- https://en.wikipedia.org/wiki/AlexNet
- https://habr.com/ru/company/nix/blog/430524/